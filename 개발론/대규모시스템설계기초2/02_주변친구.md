# 2장 주변친구
 * 앱 사용자 가운데 본인 위치 정보 접근 권한을 허락한 사용자에 한해 인근의 친구 목록을 보여주는 시스템
 * 근접성 서비스와 비슷해보이지만, 주변 친구 위치는 사업장 주소와 다르게 매번 바뀌므로 큰 차이가 있다.

## 1단계: 문제 이해 및 설계 범위 확정
### 기능 요구사항
 * 주변 친구 정보과 거리, 마지막 갱신 시간 함께 표시
 * 친구 목록은 몇초에 한번씩 갱신
### 비기능 요구사항
 * 낮은 지연시간
 * 안정성 : 몇개의 데이터가 유실되는 것은 용인 가능
 * 결과적 일관성 (강하지 않은 일관성) : 복제본 데이터가 원본과 동일하게 되기까지 몇초 걸리는 것은 용인 가능
### 개략적 규모추정
 * 주변친구는 5마일 반경으로 한다.
 * 친구 위치는 30초 주기 갱신한다.
 * 사용자는 1억명
 * 동접자는 DAU의 10% -> 1000만명.
 * 평균 1인당 400명 친구 가진다.
 * 앱 최초 조회시 20명만 보여주고 추가 요청시 더 보여준다.
 * QPS : 천만/30초 -> 334,000

## 2단계: 개략적 설계안 제시 및 동의구하기

### 개략적 설계안
 * 사용자는 근방의 모든 활성 상태 친구의 새 위치 정보를 수신하고자 한다.
 * 이론적으로는 순수한 P2P 방식으로도 해결 가능한 문제다.
 * 다시 말해, 활성 상태인 근방 모든 친구와 항구적 통신 상태를 유지하면 되는 것이다.
 * 좀 더 실용적인 설계안은 공용 백엔드 사용이지만, 규모를 키우기 어렵다는 단점이 있다.

### 설계안
#### 웹소켓 서버
 * 친구 위치 정보 변경을 거의 실시간에 가깝게 처리하는 유상태 서버 클러스터
 * 각 클라이언트는 그 가운데 한 대와 웹소켓 연결을 지속 유지한다.
 * 주변 친구 기능을 이용하는 클라이언트의 초기화도 담당한다. 모바일 클라이언트가 시작되면 온라인 상태인 모든 주변 친구 위치를 해당 클라이언트로 전송하는 역할을 한다.
#### 레디스 위치 정보 캐시
 * 활성 상태인 사용자의 가장 최근 위치 정보 캐시
#### 레디스 펍/섭 서버
 * 레디스 펍/섭은 초경량 메시지 버스다.
 * 레디스 펍/섭에서의 새로운 채널 생성은 아주 값싼 연산이다.
 * 사용자마다 채널을 생성하고, 위치가 갱신되면 채널로 이를 전송한다. 그럼 이 사용자와 구독관계인 친구들 모두가 이 정보를 수신한다.

<img src="https://github.com/jaehleeee/study-docs/assets/48814463/f39bf370-9daa-4f0d-a5b3-0786c243ff50" width="500"/>

#### 주기적 위치 갱신 과정
1. 클라이언트가 위치가 변경된 사실을 로드밸런서로 전송
2. 로드밸런서는 웹소켓 서버로 전송
3. 웹소켓 서버는 해당 이벤트를 위치 이동 이력 DB에 저장 및 위치 정보 캐시에 보관 (TTL 갱신)
4. 웹소켓 서버는 레디스 펍/섭 서버에 해당 사용자 채널에 새 위치 정보를 발행
5. 이를 구독하는 구독자에게 브로드캐스트된 위치 정보를 받은 구독자들은 이벤트를 수신
6. 수신된 정보를 받은 사용자들은 신규 거리 측정
7. 이 거리가 검색 반격을 넘지 않으면 해당 타임스탬프와 위치 정보를 구독자의 클라이언트 앱으로 전송

<img src="https://github.com/jaehleeee/study-docs/assets/48814463/a95b7246-ff91-4eb3-aa54-957a0e5fb3ea" width="500"/>

### API 설계
 * 특이사항 없음.

### 데이터 모델
 * 위치 정보 캐시는 '주변친구' 기능을 켠 활성 상태 친구의 가장 최근 위치를 보관한다

#### 위치 정보 저장에 데이터베이스를 사용하지 않는 이유는?
 * (과거말고) 현재 위치만을 이용하기 때문이다.
 * 따라서 사용자 위치는 하나만 보관하면 충분하다.
 * TTL을 지원하므로 활성 상태가 아닌 사용자 정보를 자동으로 제거할 수 있다.
 * 즉, 영속성을 보장할 필요가 없으면서 읽기 및 쓰기 연상 속도가 빠르다는 장점을 잘 활용할 수 있다.

#### 위치 이동 이력 DB
 * 막대한 쓰기 연산 부하를 감당하면서 수평적 규모 확장이 가능한 카산드라가 좋음.
 * 데이터가 많을 수 이씅니 샤딩 필요. (사용자 ID를 key 설정)

## 3단계: 상세 설계
 * 규모가 늘어날때 병목 및 해결방안을 찾는다.

### 웹소켓 서버
 * 기존 서버 제거시 주의: 노드 제거하기 전 연결 종료 후 제거 → 좋은 로드밸런서 필요

### 클라이언트 초기화
1. 연결 초기화되면, 모바일 단말의 위치정보 전송
2. 웹소켓 연결 핸들러는, 캐시 위치정보 갱신
3. 연결 핸들러 내 위치 정보 저장
4. 사용자의 모든 친구 정보 가져오기
5. 위치 정보 캐시에 일괄 요청으로 모든 친구 위치 가져오기
6. 각 친구들의 위치 정보와 사용자의 거리 계산 및 해당 정보 클라이언트에 전송
7. 각 친구의 레디스 펍/섭 채널 구독 
    - 비활성화 상태 친구 채널포함 - 유지 비용 < 활성상태 전환 비용
8. 사용자의 현재 위치를 모든 친구에게 전송

### 사용자 데이터베이스
1. 사용자 상세 정보: ID, 이름 등 기본 프로필
2. 친구 관계 데이터
→ 수평적 규모 확장이 가능하도록 적절한 샤딩이 필요

### 위치 정보 캐시
#### 레디스 활용
- 천만 사용자 * 위치 정보 100바이트 =~ 수 GB
- 334K QPS → 사용자 ID 기준 샤딩을 통해 부하 분산
- 가용성을 높이기 위해 standby 노드에 복제

### 레디스 펍/섭 서버
#### 메시지 라우팅 계층으로 활용
- 채널 생성 비용이 매우 저렴하다.
- 채널 유지를 위해서는 구독자 관계 추적을 위한 해시 테이블과 연결 리스트가 필요함 → 소량의 메모리만 사용

#### 저렴한 채널 생성/유지 비용 활용
1. ‘주변 친구’ 기능을 사용하는 모든 사용자에게 채널 부여. 초기화 시 (활성 상태와 상관없이) 모든 친구의 채널과 구독관계 설정
2. 더 많은 메모리를 사용하게 되지만, 아키텍쳐 단순화를 위해 투입할만함

### 얼마나 많은 레디스 펍/섭 서버가 필요한가?
#### 메모리 사용량
> 채널 수 = 사용자 수 = 1억개 * 100명 기능사용 * 내부 해시테이블 & 연결 리스트 20바이트 = 200GB 

#### CPU 사용량
> 위치 정보 업데이트 양: 초당 1400만 건
> 서버 한 대당 10만이라고 가정시, 1400만 / 10만 = 140대가 필요하다
→ 메모리가 아닌 CPU가 병목: 분산 클러스터가 필요하다

### 분산 레디스 펍/섭 서버 클러스터

#### 모든 채널은 서로 독립적 → 사용자ID 기준 샤딩 가능
> 서비스 탐색 service discovery 컴포넌트를 도입해 해결해보자



### 친구 관련


