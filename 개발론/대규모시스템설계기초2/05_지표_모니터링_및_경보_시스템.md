# 5장 지표 모니터링 및 경보 시스템

# 1단계: 문제 이해 및 설계 범위 확정
## 개략적 요구사항 및 가정
- 대규모 인프라 모니터링 해야함
    - DAU: 1억명
    - 서버 풀 1000개, 풀당 서버 수 100개, 서버당 100개 운영지표
        - 약 천만개의
    - 데이터 보관 1년
        - 지표 해상도 변경 허가
## 비기능 요구사항
- 규모 확장성
- 낮은 응답 지연
- 안전성
- 유연성
- 고려 하지 않아도 되는 것: 로그, 추적

# 2단계: 개략적 설계안 제시 및 동의 구하기
## 기본적 사항
- 데이터 수집
- 데이터 전송
- 데이터 저장소
- 경보
- 시각화
- (어떤 지표 수집을 할지부터 생각을 시작하되, 시각화나 경보는 어떤게 필요할지로 생각을 추리면 좋을거 같다.)
## 데이터 모델
- 통상 시계열 데이터 형태로 기록 = 값 집합에 타임스탬프가 붙은 형태로 기록
    - 선택적으로 label을 붙이기도 함.
- 데이터 접근 패턴
    - 쓰기 부하는 막대함
    - 읽기 부하는 치솟다 사라짐 (spiky)
    - (결국 긴 기간동안의 다량의 데이터를 집계하여 유의미하게 쿼리가 되려면 이 순간적인 읽기 부하를 잘 지원해주는게 필요할거 같다.)
## 데이터 저장소 시스템
- 직접 설계 비추
- Mysql 비추
    - 이론적으로는 가능하지만, 전문가 수준의 튜닝이 필요.
        - e.g. 지수 이동 평균값을 지속적으로 갱신하는 질의문이 복잡하고 가독성도 안좋음
    - 태그/레이블별로 인덱스 지정도 필요
    - 쓰기 전용도 아님
- Nosql 비추
    - e.g. 카산드라, 빅테이블
        - 내부 구조에 대한 해박한 지식 필요
- 시계열 데이터 저장소 추천
    - 시계열 데이터에 성능 최적화 가능
    - 쉬운 질의 인터페이스
    - 보관 기관 설정과 데이터 집계 기능도 제공
    - OpenTSDB: 하둡과 HBase 기반하고 있어 운영이 복잡
    - MetricsDB: X(twitter) 사용
    - Timestream: 아마존 사용
    - 시장에서 가장 인기있는 데이터베이스: InfluxDB, Prometheus
        - 대용량 저장 가능
        - 빠른 실시간 분석 지원
        - 메모리 캐시와 디스크 저장소를 함꼐 사용
        - 영속성 요건과 높은 성능 요구사항도 만족
        - e.g. 8CPU + 32GB InfluxDB 는 초당 250,000 회 쓰기 연산 처리 가능
        - 레이블: 인덱스 구축 및 과부하 피하기 지침 제공
            - cardinality
            - (N2c Prometheus 에서도 cardinality 이슈로 조회가 안되거나 혹은 app서버 메모리 부족 이슈 발생함)
## 개략적 설계안
<img src="https://github.com/user-attachments/assets/83cf5bfa-4e61-432a-bc2c-552409131033" width="500"/>

- 지표 출처: 지표 데이터가 만들어지는 곳
- 지표 수집기: 지표 데이터 수집하고 시계열 데이터에 기록
- 시계열 데이터베이스: 다량의 시계열 데이터를 분석하고 요약하는 데 적합하도록 설계된 질의 인터페이스를 제공
- 질의 서비스: 시계열 데이터베이스에 보관된 데이터를 질의하고 가져오는 과정을 돕는 서비스
- 경보 시스템: 경보를 받아야하는 다양한 대상으로 경보 알림을 전송하는 역할을 하는 시스템
- 시각화 시스템: 지표를 다양한 형태의 그래프/차트로 시각화하는 기능을 제공하는 시스템

# 3단계: 상세 설계
## 지표수집
- 지표 출처와 지표 수집 부분
### 풀 vs 푸시 모델
- 풀 모델
    - HTTP 기반 풀 모델을 이용하는 지표 수집 흐름
    - 서비스 목록을 알아야함
        - etcd나 아파치 주키퍼 같은 서비스 탐색 기술을 활용하면 문제 해결 가능
        - 가용성 관련 정보를 서비스 탐색 서비스에 기록하고
        - SDS는 서비스 엔드포인트 목록에 변화가 생길 때마다 지표 수집기에 통보
        - (그림 5.10 참고)
            - 지표 수집기 SDS에서 서비스 엔드포인트 설정 메타데이터 목록을 가져옴
                - 지표 수집주기, IP 주소, 타임아웃, 재시도 인자 등 기록되어있음
            - 사전에 합의된 endpoint에서 지표를 조회해옴
                - 통상 서비스는 특정 라이브러리를 이용해 동일한 지표용 endpoint를 제공함
            - 지표 수집기는 서비스 엔드포인트 목록의 변화를 통지 받기위해 이벤트 알림 콜백을 서비스 탐색 컴포넌트에 등록하거나(푸시) 주기적으로 데이터 목록을 가져오거나 함.
    - 지표 수집기 서버 풀을 만들어야 데이터 규모 감당 가능
        - 데이터 enpoint와 지표 수집기 서버가 N대 M의 관계가 형성됨
        - 안전 해시링을 사용
     
- 푸시 모델   
    - 직접 지표를 수집기에 전송하는 모델
        - 통상 에이전트를 설치하여 지표 전달
            - 간단한 카운터 지표의 경우에는 수집기에 보내기전에 집계 등의 처리도 가능
                - 보내는 데이터량을 다이어트
            - 에이전트 내부의 버퍼를 이용해 일시적 데이터 보관도 가능, 소실 가능성도 충분히 있음
        - 로드밸런서를 두어 지표 수집기 서버 CPU 부하를 감당
- 풀 vs 푸시 모델 비교
    - 풀 모델: prometheus
        - 디버깅, 상태진단, 데이터 신빙성 측면엔서 풀모델이 낫다
            - 인증을 강제하면 데이터 신빙성은 푸시모델도 괜찮다.
    - 푸시 모델: graphite, cloudwatch
        - 생존기간이 짧은 프로세스, 방화벽 등의 복잡한 네트워크, 성능(UDP)면에서 푸시모델이 낫다.
    - (표 5.3 참고)
<img src="https://github.com/user-attachments/assets/5bb654a1-852e-4796-bc96-1457fbe8eb40" width="500"/>

## 지표 전송 파이프라인의 규모 확장
- 지표 수집기 및 시계열 데이터베이스 부분
- 큐를 통해 규모 확장에 용이하게 설정하면서도 장애 방지
    - e.g. storm,  flink, spark
- (그림 5.15 큐 추가)

### 카프카를 통한 규모 확장
- 대역폭 요구사항에 따라 파티션 수 설정 가능
- 지표 분류 가능
    - 지표 이름에 따라 데이터 집계 가능
    - 태그/레이블에 따라
- 카프카의 대안
    - 큐 없이도 대규모 데이터 처리가 가능한 모니터링 시스템: 페이스북의 고릴라

## 데이터 집계 지점
- 수집 에이전트가 집계하는 방안
    - 간단한 카운터 값을 분 단위로 집계하여 지표 수집기에 보내는 정도 가능
        - (플레이어의 qoe)
- 데이터 수집 파이프라인에서 집계하는 방안
    - 스트림 프로세싱 엔진이 필요
    - 늦게 도착하는 지표 데이터의 처리가 어렵고
        - (cuve데이터 처리시나 data팀과 연계할 때 어려운 부분 - 데이터 집계의 신뢰도가 필요할까?)
    - 원본 데이터 보관하지 않아 정밀도나 유연성 측면에서 손해를 보게 된다
- 질의 시에 집계하는 방안
    - 날것 그대로 보관
    - 속도가 느린게 문제
- (데이터 수집 파이프라인에서 입맛에 맞게 여러방안으로 집계하는게 제일 합리적이지 않나 생각)
## 질의 서비스
### 캐시 계층
- 질의 부하를 낮출 수 있음
### 질의 서비스를 두면 곤란한 경우
- 시각화 시스템에서 강력한 플러그인에서 제공해주기도 함.

## 저장소 계층
시계열 데이터베이스는 신중하게 선택할 것
- 페이스북에 따르면 질의의 85%는 26시간 내에 수집된 데이터임

저장 용량 최적화
- 데이터 인코딩 및 압축
    - 이중-델타 인코딩 하면 기준값과 차이를 저장하는 방식
- 다운샘플링
    - 데이터의 해상도를 낮춰 저장소 요구량을 줄이는 기법
- 냉동 저장소
    - cold storage는 잘 사용되지 않는 비활성 상태 데이터를 보관하는 곳
## 경보 시스템

- 설정 파일을 가져와 캐시 서버에 보관
    - 경보 규칙은 보통 디스크에 저장, 규칙을 정의하는 것은 YAML 형식
- 경보 관리자는 경보 설정 내역을 캐시에서 가져옴
- 설정된 규칙에 근거하여 경보 관리자는 지정된 시간마다 질의 서비스를 호출
    - 임계값 위반시 경보 이벤트 생성
    - 경보 필터링, 병합, 중복 제거
    - 접근 제어
        - 특정한 개인만이 수행할 수 있도록 제한
    - 재시도
        - 알림이 최소 한번 전달됨 보장해야함
- 경보 저장소는 경보의 상태값이 저장됨
    - 키-값 저장소 e.g. 카산드라
- 경보이벤트를 카프카에 전달
- 경보 소비가자 이벤트를 읽고 처리
- 경보 시스템 - 만들 것인가 구매할 것인가
    - 만드는게 합리적
## 시각화 시스템
- 상용품을 구입해서 쓰자고 주장하는 것이 바람직하다.

# 4단계: 마무리
## 최종 설계안
<img src="https://github.com/user-attachments/assets/b2398d9a-dc8d-4574-9c55-070a6b80eb74" width="500"/>

- 지표 데이터 수집 모델: 풀 모델 vs 푸시 모델
- 카프카를 활용한 규모 확장 방안
- 최적 시계열 데이터베이스의 선정
- 다운샘플링을 통한 데이터 크기 절감
- 경보/시각화 시스템: 구현할 것인가 구입할 것인가


