- 결제 플랫폼은 일반적으로 고객에게 전자 지갑 서비스를 제공.
- 결제 이외에도, 다른 사용자 지갑으로 직접 송금을 지원

# 1단계 : 문제 이해 및 설계 범위 확정

- 이체에만 집중
- 1,000,000 TPS
- DB가 제공하는 트랜잭션 보증이면 충분
- 재현성을 갖춘 시스템을 설계
    - 처음부터 데이터를 재생하여, 언제든지 과거 잔액을 재구성할 수 있어야 함
- 가용량 99.99%

## 개략적 추정

- 관계형 DB는 수천 건의 트랜잭션 처리 가능
- 본 설계에서는 Node가 1000TPS 지원한다고 가정
- 1백만 TPS → 1000개의 DB노드가 필요 ?
    - 인출, 입금 2건이므로 2000개의 노드
- 한 노드가 초당 처리할 수 있는 트랜잭션 수가 많을수록 필요한 총 노드 수는 줄어들게 됨.

<aside>
💡

단일 노드가 처리할 수 있는 트랜잭션 수를 늘리는 것이 설계 목표 중 하나

</aside>

# 2단계 : 개략적 설계안 제시 및 동의 구하기

- API는 1개만 존재. from ~ to 이체

## 인메모리 샤딩
- 지갑 애플리케이션은 모든 사용자 계정의 잔액을 유지한다.
- <사용자, 잔액> 을 나타내기에 좋은 자료 구조는 KV Store
→ 레디스 노드 한 대로 100만 TPS는 어려움. 
→ 파티셔닝/샤딩이 필요




<aside>
💡
모든 레디스 노드의 파티션 수 및 주소는 한군데에 저장
높은 가용성 보장되는 ZooKeeper를 이 용도로 쓰면 좋다.
</aside>

이 지갑 서비스는 다음과 같은 중요한 역할을 담당한다.

1. 이체 명령의 수신
2. 이체 명령의 유효성 검증
3. 명령이 유효하다면, 두 계정의 잔액 갱신. 이 두 계정은 서로 다른 레디스 노드에 있을 수 있다.




- 무상태 서비스고, 수평적 규모 확장이 용이

<aside>
🔥

이 설계는 작동은 하지만, 레디스 노드 업데이트의 원자적 트랜잭션 보장이 되지 않음.

</aside>

---

## 분산 트랜잭션

### 데이터베이스 샤딩

- 트랜잭션을 지원하는 RDBMS로 교체할 수 있지만, 그럼에도 두 작업이 동시에 처리된다는 보장이 없음

### 분산 트랜잭션 : 2단계 커밋

- 분산 트랜잭션의 구현법으로는 저수준 방안과 고수준 방안 2가지가 있음
- 저수준은 DB 자체에 의존하는 방안
- 이때 보통 2단계 커밋 (2PC)를 사용 → 두 단계로 실행





<aside>
💡
락을 걸고, 준비 단계를 OK 받으면, 커밋을 실시
어느 한 DB라도 아니요를 응답하면 조정자는 모든 데이터베이스에 트랜잭션 중단을 요청한다.
</aside>

**단점**

- 조정자가 SPOF
- 모든 데이터베이스가 X/Open XA 표준을 만족해야함
- 다른 노드의 메시지를 기다리는 동안 락이 오랫동안 잠겨서 성능 이슈가 존재

---
### 분산 트랜잭션 : TC/C
- Try-Confirm/Cancel 은 2단계로 구성된 보상 트랜잭션

<aside>
💡
- 자원 예약 요청
- 모두 ‘예’ 라고 응답하면 작업 확인을 요청 (Try-Confirm)
- 어느 하나라도 ‘아니요’ 라고 응답하면 작업 취소를 요청. (Try-Cancel)
2PC의 두 단계는 하나의 트랜잭션이지만, TC/C에서는 각 단계가 별도 트랜잭션이라는 점이 차이
</aside>

TC/C 사례

- 계좌 A → C로 1달러를 이체한다고 가정





- 아무것도 하지 않을때는 NOP 명령을 내린다고 한다.

첫번째 단계 




- A는 트랜잭션이 존재

두번째 단계 (확정 단계)






두번째 단계 (취소 단계)






- C 계정에 대한 NOP이 사실은 불법 계정 등으로 인해 실패할 수 있음

2PC와 TC/C의 비교

- 2PC는 2번째 단계가 시작될 때 모든 로컬 트랜잭션이 완료되지 않은 상태 (락도 잠겨있음)
- TC/C는 모든 로컬 트랜잭션이 완료, 락도 없음
    - TC/C는 새로운 보상 트랜잭션을 실행

<aside>
💡

TC/C는 보상 기반 분산 트랜잭션. 실행 취소를 비즈니스 로직으로 구현하므로, 고수준 해법.

DB에 구애받지 않음. 트랜잭션을 지원하기만 하면 된다.
단점 → 애플리케이션 비즈니스 로직에서 세부 상황 관리하고, 분산 트랜잭션의 복잡성을 처리해야 한다는 것

</aside>

### 단계별 상태 테이블

- TC/C 실행 도중에 지갑 서비스가 다시 시작되면 어떻게 되나?

<aside>
💡

해결책은 진행 상황 (특히 각 단계 상태 정보) 을 트랜잭션 데이터베이스에 저장하면 된다.

</aside>

- 분산 트랜잭션의 ID와 내용
- 각 DB에 대한 시도 단계의 상태 (not sent yet, has been sent, response received) 의 3가지 값 중 하나
- 두 번째 단계의 이름 Confirm, Cancel 중 하나
- 두 번째 단계의 상태
- 순서가 어긋났음을 나타내는 플래그

<aside>
💡
어디에 이 데이터를 저장하는 것이 좋을까? 
→ 일반적으로 돈을 인출할 지갑의 계정이 있는 DB
</aside>






### 불균형 상태

시도 단계가 끝나고 나면 $1가 사라진다. 거래 후에도 잔액 총합은 동일해야 한다는 회계 기본 원칙을 위반한다.

다행히, TC/C는 여러개의 독립적인 로컬 트랜잭션으로 구성. 

- 실행 주체가 애플리케이션.
- 중간 결과를 볼 수 있음
- DB 트랜잭션이나, 2PC 같은 분산 트랜잭션은 실행 주체가 DB고, 애플리케이션은 중간 결과를 볼 수 없음

<aside>
💡
분산 트랜잭션 도중에는 항상 데이터 불일치가 발생. TC/C와 같은 메커니즘을 사용하는 경우에는 우리가 직접 처리해야 함
</aside>






### 유효한 연산 순서

시도 단계에서 할 수 있는 일은 세 가지






- 선택2는, C는 성공하였으나 A에서 실패한 경우 → 취소 단계를 실행해야 하는데, 누군가 C 에서 그 사이에 $1를 이미 이체하였으면?
    - 1달러를 차감하려고하면, 아무것도 안남을텐데. 트랜잭션 보증 위반
- 선택3은, 많은 문제가 발생. C에 1은 추가되고, A에서 차감만 실패하면?

<aside>
💡

2, 3은 유효하지 않음. 1번만이 올바른 방법

</aside>

### 잘못된 순서로 실행된 경우

TC/C에는 실행순서가 어긋날 수 있다는 문제가 있음.





- 네트워크 이슈로 시도가 나중에 들어옴

<aside>
💡

취소 명령이 먼저 도착하면 DB에 상응하는 시도 명령을 못 보았음을 나타내도록 플래그 설정

시도 도착 후, 먼저 도착한 취소 명령이 있었는지 확인 후 바로 실패

앞의 단계별 상태 테이블에서 플래그를 마련해 둔 이유

</aside>

## 분산 트랜잭션 : 사가

### 선형적 명령 수행

1. 모든 연산은 순서대로 정렬. 각 연산은 자기 데이터베이스에 독립 트랜잭션으로 실행
2. 연산은 첫 번째부터 마지막까지 순서대로 실행. 한 연산이 완료되면 다음 연산이 개시
3. 연산이 실패하면, 전체 프로세스는 실패한 연산부터 맨 처음 연산까지 역순으로 보상 트랜잭션을 통해 롤백






<aside>
💡

어떻게 연산 실행 순서를 조율할까?

</aside>

1. 분산 조율 (Choreography , 안무) : 관련된 모든 서비스가 다른 서비스의 이벤트를 구독하여 작업을 수행하는 방식
2. 중앙 집중형 조율 (Orchestration) : 하나의 조정자가 모든 서비스가 올바른 순서로 작업을 실행하도록 조율

<aside>
⚠️

PRISM은 DB를 완전히 분리하지 못하기도했고, 순서 조율이 필요해서 Orchestration 을 위한 livecycle 이라는 모듈을 만들어서 설계

</aside>

<aside>
💡

둘중 어떤걸 사용할지는 사업상의 필요와 목표에 따라 정함

</aside>

| 분산 조율 방식 | 중앙 집중형 조율 |
| --- | --- |
| * 서로 비동기식으로 통신하기 때문에, 모든 서비스는 이벤트의 결과로 어떤 작업을 수행할지 정하기 위해 내부적으로 상태 머신이 필요 | * 복잡한 상황을 잘 처리하기 때문에, 일반적으로는 선호 |

---

### TC/C vs 사가






- 사가는 지연시간이 있다. 병렬 실행이 불가능
- 지연 시간 요구사항이 없거나, 서비스 수가 매우 적다면 아무것이나
- 아니면 TC/C

<aside>
🔥

분산 트랜잭션 방안도 제대로 작동하지 않을 수 있는데 (애플리케이션 수준에서 사용자의 잘못), 문제의 근본 원인을 역추적하고 모든 계정에서 발생하는 연산을 감사할 방법이 있다면 좋을 텐데. 어떻게?

</aside>

## 이벤트 소싱

실제로 감사를 받으면 다음과 같이 까다로운 질문들을 던질 수 있음

1. 특정 시점의 계정 잔액을 알 수 있나요?
2. 과거 및 현재 계정 잔액이 정확한지 어떻게 알 수 있나요?
3. 코드 변경 후에도 시스템 로직이 올바른지는 어떻게 검증하나요?

<aside>
💡

DDD에서 개발된 기법인 이벤트 소싱이 적합

</aside>

이벤트 소싱에는 4가지 중요한 용어가 있음

1. 명령
2. 이벤트
3. 상태
4. 상태 기계

---

### 명령

- 외부에서 전달된, 의도가 명확한 요청
- 순서가 매우 중요. FIFO 큐에 저장

### 이벤트

- 명령은 유효하지 않고, 실패할 수 있음
- 검사를 통과한 명령은 반드시 이행 되어야 함.
- 이 결과가 이벤트

<aside>
💡

이벤트는 검증된 사실로, 실행이 끝난 상태. 과거 시제를 사용. 완료하였음. 등

이벤트는 결정론적. 과거에 실제로 있었던 일.

</aside>

- 하나의 명령으로 여러 이벤트가 만들어질 수 있음 (0이상)
- 이벤트 생성 과정에는 무작위성이 개입될 수 있어서, 같은 명령에 항상 동일한 이벤트들이 만들어진다는 보장이 없음. (외부 I/O, 난수가 개입될 수 있음)

### 상태

이벤트가 적용될 때 변경되는 내용. 지갑 시스템에서는 상태는 모든 클라이언트 계정의 잔액. 

map을 자료구조로 사용. 보통 키-값 저장소를 사용 (RDBMS도 사용가능)

### 상태 기계

실제 이벤트 소싱 프로세스를 구동. 

1. 명령의 유효성을 검사하고 이벤트를 생성
2. 이벤트를 적용하여 상태를 갱신

<aside>
💡

상태기계는 결정론적으로 동작해야 함. 무작위성 X, I/O 를 통한 무작위적 데이터 읽기, 난수 사용 X. 
이벤트를 상태에 반영하는 것 또한 항상 같은 결과를 보장

</aside>






## 지갑 서비스 예시

- 명령 - 이체 요청
- FIFO 큐로는 카프카를 널리 사용






계정 잔액은 RDBMS. 상태 기계는 명령을 큐 순서대로 확인. 

계정에 충분한 잔액이 있는지 유효성 검사. 그리고 이벤트를 만든다.

<aside>
💡

명령이 A → $1 → C 라면, 이벤트는 A: -$1, C: +$1 와 같은 두 이벤트를 만든다.

</aside>






### 재현성

- 이벤트 소싱의 장점 → 재현성 (Reproducibility)
- 앞서 언급한 분산 트랜잭션 방안의 경우, DB에 저장. 변경된 이유를 알기가 어렵고, 과거 잔액을 알기 어렵고, 특정 시점의 잔액만 보여줌
- 이벤트는 처음부터 다시재생하면 과거 잔액 상태는 언제든 재구성 할 수 있다.
- 이벤트 리스트는 불변. 상태 기계는 결정론적. 재생하여 만든 상태는 언제나 동일.

앞선,

1. 특정 시점의 계정 잔액을 알 수 있나요?
2. 과거 및 현재 계정 잔액이 정확한지 어떻게 알 수 있나요?
3. 코드 변경 후에도 시스템 로직이 올바른지는 어떻게 검증하나요?

를 다 답할 수 있음.

<aside>
💡

감사 가능성 때문에, 이벤트 소싱이 지갑 서비스 구현의 실질적인 솔루션으로 채택되는 경우가 많음.

</aside>

### CQRS (명령 - 질의 책임 분리)

효과적인 계좌 이체가 가능한 지갑 서비스를 설계. 하지만, 클라이언트는 계정 잔액을 알 수 없음. 

→ 상태(잔액)을 알도록 할 방법이 필요. 

상태 이력 데이터베이스의 읽기 전용 사본을 생성?

<aside>
💡

이벤트 소싱은 이와는 조금 다른 해결책을 제시

→ 계정 잔액의 공개 대신, 모든 이벤트를 외부에 보내어, 이벤트를 수신하는 외부 주체가 직접 상태를 재구축하도록 함.

CQRS.

읽기 전용 상태 기계는 상태 뷰를 만들고, 이 뷰는 질의에 이용됨.

</aside>

읽기 전용 상태 기계는 실제 상태에 어느정도 뒤쳐질 수 있으나, 결국에는 같아짐. → Eventual consistency 모델





<aside>
🔥

제안한 이벤트 소싱 아키텍처는 한 번에 하나의 이벤트만 처리. 여러 외부 시스템과 통신

더 빠르게 만들 수 있을까요?

</aside>

---

# 3단계 : 상세 설계

## 고성능 이벤트 소싱

### 파일 기반의 명령 및 이벤트 목록

- 카프카 대신 로컬 디스크에 저장.
- 네트워크 전송 시간을 아낌
- 이벤트 목록은 추가 연산만 가능 → 순차적 쓰기, HDD에서도 가능
- 최근 명령과 이벤트를 메모리에 캐시하는 방안도 가능.
- 구체적으로는 mmap 기술을 통해서 최적화 구현이 가능

<aside>
💡

로컬 디스크에 쓰는 동시에, 최근 데이터는 메모리에 자동으로 캐시할 수 있음. 디스크 파일을 메모리 배열에 대응시킴 (mmap)

→ 카프카 페이징이랑 같은 원리

</aside>

### 파일 기반 상태

이전과 다르게, RDBMS를 쓰지않고, 상태 정보도 로컬 디스크에 저장.

SQLite, RocksDB 를 사용

여기서는 RocksDB를 사용. 쓰기에 최적화된 자료 구조 LSM 을 사용

<aside>
⚠️

카프카 Streams 처럼 사용

</aside>







### 스냅숏

모든 것이 파일 기반일 때, 재현 프로세스의 속도를 높일 방법은?

→ 재현성은 이벤트를 처음부터 다시 읽는 대신, 주기적으로 상태 기계를 멈추고 현재 상태를 파일에 저장한다면? → 시간 절약

<aside>
💡

스냅숏

</aside>

지갑 서비스 같은 금융 애플리케이션은 00:00에 스냅숏을 찍는 일이 많음. 스냅숏을 사용하면, 읽기 전용 상태 기계는 해당 데이터가 포함된 스냅숏 하나만 로드하면 됨.

- 스냅숏 → 거대한 binary file, HDFS 같은 객체 저장소에 저장

<aside>
⚠️

S3, Object Storage도 적절하지 않을까?

</aside>







<aside>
💡

모든 것이 파일 기반일 때 시스템은 컴퓨터 하드웨어 I/O 처리량을 그 한계까지 최대로 활용할 수 있음

</aside>

<aside>
🔥

좋다고 할 수 있지만, 더 이상 무상태 서버가 아니고, SPOF가 될 수 있음. 안정성은?

</aside>

---

## 신뢰할 수 있는 고성능 이벤트 소싱

### 신뢰성 분석

데이터의 신뢰성이 훨씬 중요. 계산 결과는 다른 노드에서 돌리면 복구할 수 있음. 

이 시스템의 데이터는,

1. 파일 기반 명령
2. 파일 기반 이벤트
3. 파일 기반 상태
4. 상태 스냅숏

이 존재

상태와, 스냅숏은 → 언제든 다시 만들 수 있음. 

이벤트 목록의 신뢰성만 보장하면 된다. 

<aside>
💡

명령 → 이벤트 생성은 결정론적 과정이 아님, 난수나 외부 입출력 등의 무작위적 요소가 포함될 수 있음. 즉, 명령의 신뢰성만으로는 재현성 보장이 어려움. 

</aside>

### 합의

높은 안정성을 제공하려면, 이벤트 목록을 여러 노드에 복제해야 한다. 복제 과정은 다음을 보장해야 한다.

1. 데이터 손실 없음
2. 로그 파일 내 데이터의 상대적인 순서는 모든 노드에 동일

<aside>
💡

합의 기반 복제 방안이 적합. 모든 노드가 동일한 이벤트 목록에 합의하도록 보장. 
래프트 알고리즘을 예로 들어본다. 

</aside>

### 래프트 알고리즘

<aside>
💡

노드의 절반 이상이 온라인 상태면, 그 모두에 보관된 추가 전용 리스트는 같은 데이터를 가짐.

</aside>

1. 리더
2. 후보
3. 팔로워

가 존재. 

### 고신뢰성 솔루션






<aside>
🔥

100만 TPS를 처리하려면 서버 한 대로는 충분하지 않다. 어떻게 해야 시스템의 확장성을 높일 수 있을까?

</aside>

### 분산 이벤트 소싱

앞선 아키텍처는 신뢰성 문제는 해결하지만, 다른 문제가 있음

1. CQRS 시스템에서는 요청/응답 흐름이 느릴 수 있음.
    - 클라이언트가 지갑의 업데이트 시점을 정확히 알 수 없어서, polling에 의존해야 할 수 있기 때문
2. 단일 래프트 그룹의 용량은 제한되어 있다. 일정 규모 이상에서는 데이터를 샤딩하고 분산 트랜잭션을 구현해야 한다.

<aside>
⚠️

복제 자체를 Raft로 하는건 좋지 못한 것 같음. RabbitMQ의 Quorum Queue가 TPS가 잘 나오지 못하는 이유. Kafka처럼 metadata 관리는 Raft, 실제 Copy는 자체 프로토콜을 가져가는게 더 적합하다고 판단.

</aside>

### 풀 vs 푸시





풀 모델에서는 외부 사용자가 읽기 전용 상태 기계에서 주기적으로 읽음.

비 실시간, 주기가 너무 짧으면 지갑 서비스에 과부하가 걸릴 수도 있음

- 이때, 리버스 프록시를 앞단에 두어서, 클라이언트의 로직을 단순화 할 수 있음
    - 여전히 실시간 X





그 후에, 한번 더 읽기 전용 상태 기계를 수정해서 응답 속도를 높일 수 있음.

<aside>
💡
읽기 전용 상태 기계가 자기만의 특별한 로직을 가질 수 있음. 이벤트를 수신하자마자 실행 상태를 Reverse Proxy에 푸시하도록 하면? 실시간으로 푸시 기반으로 응답이 이루어지는 느낌을 줄 수 있음
</aside>





<aside>
이벤트 기반 시스템에서는, 결국 최초 요청자에게 Feed back을 주기가 어렵다는 단점이 존재한다. Callback을 주는 방식이 필요한데, 사내에서 플라즈마는 ws으로 줬던 기억이 있고, PRISM은 MQTT가 그 용도로 쓰인다.
이 사례처럼, 채널을 하나로 묶으면 더 클라이언트 입장에서 인터페이스가 단순해 질 수 있겠다는 생각이 든다.
</aside>

### 분산 트랜잭션

모든 이벤트 소싱 노드 그룹이 동기적 실행 모델을 채택하면, TC/C나 Saga 같은 분산 트랜잭션 솔루션을 재사용할 수 있다. 

- 여기서는 key의 해시값을 2로 나누어 파티션 지정





### 최종 예제





# 4단계 : 마무리

- 초당 100만건 이상의 결제 명령을 처리할 수 있는 지갑 서비스 설계
- 시스템 규모 추정 결과, 이 정도의 부하를 감당하려면 수천 개 노드가 필요하다는 결론.
- 첫 번째 설계안에서는 레디스 사용
    - 데이터 내구성 X
- 두 번째 트랜잭션 DB로 변경 → 2PC, TC/C, Saga.
    - 데이터 감사가 어려움
- 세 번째 이벤트 소싱 → 외부 DB와 큐 사용
    - 성능 이슈
    - 명령, 이벤트, 상태를 로컬에 저장하도록 개선
    - SPOF 이슈가 있어서, 래프트 알고리즘으로 복제
- 마지막 CQRS 개념 도입, 비동기를 동기처럼 보여주기 위해 Reverse Proxy 추가, 다시 TC/C 혹은 Saga를 도입하여 여러 노드간 명령 실행 조율도 추가

<aside>
⚠️

Saga + DDD + EventSourcing + CQRS를 위한 프레임워크도 존재. 이벤트를 소싱해주고, 제어해주는 Axon Server 라는게 제공.

[Axon Framework로 Orchestration-based Saga 구현하기!](https://jaehoney.tistory.com/403)

</aside>
