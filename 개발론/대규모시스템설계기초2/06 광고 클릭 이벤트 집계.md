### 온라인 광고 핵심 개념

- 실시간 데이터를 통해 광고 효과를 정량적으로 측정할 수 있다.
    - Click-Through Rate 클릭률
    - Conversion Rate 전환률
- Real Time Bidding 실시간 경매를 통해 광고 지면이 거래된다.
    - 속도가 중요함 - 1초내에 모든 프로세스가 진행

# 1단계: 문제 이해 및 설계 범위 확정

- 입력 데이터 형태: 분산 로그 파일
    - 수집될 때마다 로그 파일 끝에 추가
    - 클릭 이벤트 속성: ad_id, click_timestamp, user-id, ip, country
- 데이터 양: 매일 10억개 클릭, 2백만 회 광고 게재, 매년 30%씩 클릭 증가
- 지원해야 하는 질의
    - 특정 광고에 대한 지난 M분간의 클릭 이벤트 수
    - 지난 n분간 가장 많이 클릭된 광고 m개 - 집계는 매 분 이루어짐
    - ip, user_id, country 기준으로 위의 질의를 필터링 할 수 있어야 함
- 엣지 케이스 고려
    - 예상보다 늦게 도착하는 이벤트
    - 중복된 이벤트
    - 시스템 다운시 복구
- 지연 시간 요건
    - RTB: 1초 미만
    - 광고 클릭 이벤트 집계: 몇 분 정도 지연 허용

### 기능 요구사항

- 지난 M분 동안의 ad_id 클릭 수 집계
- 매분 가장 많이 클릭된 상위 100개 광고 아이디를 반환
- 다양한 속성에 따른 집계 필터링을 지원
- 데이터의 양은 페이스북이나 구글 규모(자세한 시스템 규모 요구사항은 아 래 개략적 추정치에 관한 절 참고)

### 비기능 요구사항

- 집계 결과 정확성은 데이터가 RTB 및 광고 과금에 사용되므로 중요
- 지연되거나 중복된 이벤트를 적절히 처리할 수 있어야 함
- 견고성(reliability): 부분적인 장애는 감내할 수 있어야 함
- 지연 시간 요구사항: 전체 처리 시간은 최대 수 분을 넘지 않아야 함

### 개략적 추정

- DAU 10억명 * 평균 1개 클릭 = 10억 건 이벤트
- 광고 클릭 QPS = 10^9 / 하루 10^5초 = 10,000
- 최대 QPS 5배로 가정 = 50,000QPS
- 이벤트당 0.1KB 용량 필요 → 0.1KB * 10억 = 100GB 월간 3TB

# 2단계: 개략적 설계안 제시 및 동의 구하기

## 질의 API 설계

### API1: 지난 M분간 각 ad_id에 발생한 클릭 수 집계

> GET /v1/ads/{:ad_id}/aggregated_count
> 
- from 집계 시작 시간 default: 1분전
- to 집계 종료 시간 default: 현재시간
- filter 필터링 전략 식별자 long

응답

- ad_id 광고 식별자
- count 집계된 클릭 횟수

### API2: 지난 M분간 가장 많은 클릭이 발생한 상위 N개 ad_id 목록

> GET /v1/ads/popular_ads
> 
- count 반환할 갯수
- window 분 단위 집계 윈도우 크기
- filter 필터링 전략 식별자 long

응답

- ad_ids 광고 식별자 목록 array

## 데이터 모델

원시 데이터raw data와 집계 결과 데이터aggregated

### 원시 데이터

`[AdClickEvent] ad001, 2021-01-01 00:00:01, user 1, 207.148.22.22, USA`

### 집계 결과 데이터
 * 매분 집계된다고 가정했을 때
 * M분 동안 가장 많이 클릭된 상위 N개 반환

### 비교

![image](https://github.com/user-attachments/assets/a70096dc-fe9b-4e85-817a-16a281b724f1)

> 둘 다 저장할 것을 추천

#### 원시데이터
- 디버깅에 활용: 데이터 손상시 집계 결과를 다시 만들 수 있다.
- 백업 데이터로 활용

#### 집계 결과 데이터
- 효율적인 질의
- 질의 성능을 높이기 위해 튜닝

### 올바른 데이터베이스의 선택
- 데이터의 모습: 관계형, 문서, 이진 대형 객체 BLOB
- 작업 흐름: 읽기 중심, 쓰기 중심, 둘 다
- 트랜잭션 지원여부
- 질의 과정에서 SUM, COUNT와 같은 온라인 분석 처리 OLAP 함수를 많이 사용하는가?

#### 원시데이터
- 대규모 쓰기 중심, 시간 범위 질의: 카산드라 / influxDB
- ORC, 파케이(Parquer), AVRO 와 같은 칼럼형 데이터 형식 도 사용할 수 있다.

#### 집계 데이터
- 읽기, 쓰기 모두 많이 발생: 원시 데이터 DB와 같은 유형 활용 가능

## 개략적 설계안
![image](https://github.com/user-attachments/assets/544f4ab0-7743-495d-9294-8134051502a3)

### 비동기 처리
 * 생산자와 소비자 용량이 항상 같을 수 없으므로, 메시지 큐를 도입해 결합을 끊는다.
 * 로그 모니터 - 집계 서비스 - DB : 2개의 메시지 큐로 분리
 * 집계 결과를 정확하게 한 번 처리하기 위해 atomic commit 두 번째 메시지 큐 도입
![image](https://github.com/user-attachments/assets/ab80cfe8-de7c-4707-a383-b3b3ca125c00)

## 집계 서비스
 * 광고 클릭 이벤트 집계를 위해 맵리듀스 (Map-Reduce) 프레임워크를 사용하면 좋다.
 * 모델: 유향 비순환 그래프 directed acyclic graph DAG

### 맵, 집계, 리듀스 노드로 세분화

### 맵 노드
 * 데이터를 필터링하고 변환한다.
 * 카프카 파티션/태그를 구성해서 집계 노드가 해당 카프카를 구독하게 할 수도 있다.
   → but 입력 데이터를 정리하거나 정규화 하기 위해 필요하다
 * 동일한 ad_id가 서로 다른 카프카 파티션에 입력될 수도 있다.

### 집계 노드
 * ad_id 별 광고 클릭 이벤트 수를 매 분 메모리에 집계한다. (사실상 리듀스의 일부)

### 리듀스 노드
 * 집계 노드가 산출한 결과를 최종 결과로 축약한다.
 * DAG는 맵리듀스 패러다임을 표현하기 위한 모델.
 * 중간 데이터는 메모리에 저장될 수 있고,
 * 노드 간 통신은 TCP로 처리하거나(다른 프로세스) 공유 메모리(다른 스레드)로 처리할 수 있다.

## 주요 사용 사례

### 사례1: 클릭 이벤트 수 집계
 * ad_id % 3 을 기준으로 분배

### 사례2: 가장 많이 클릭된 상위 N개 광고 반환
 * ad_id 기준으로 분배 후 각각 상위 N개를 식별 → 리듀스는 집계 노드에서 전달 받은 N*M개 중 상위 N개 식별
![image](https://github.com/user-attachments/assets/59c01fca-a3d4-4178-90d2-e4e81c40ff5a)


### 사례3: 데이터 필터링
 * 필터링 기준을 사전에 정의하고, 그에 따라 집계

#### 스타 스키마 star schema 기법
- 데이터 웨어하우스에서 널리 쓰임
- 필터링에 사용되는 필드를 차원 dimension 이라고 부름

#### 장점
- 이해하기 쉽고 구축하기 간단하다
- 기존 집계 서비스를 재사용해서 더 많은 차원을 생성할 수 있다
- 결과를 미리 계산해두므로 빠르게 접근할 수 있다

#### 단점
- 많은 버킷과 레코드가 생성된다

# 3단계: 상세 설계
## 스트리밍 vs 일괄 처리
![image](https://github.com/user-attachments/assets/4b0bee21-6c77-4a88-be6c-c433bf21c56e)

#### 람다 lambda 아키텍처
- 일괄 및 스트리밍 처리 경로를 동시에 지원하는 시스템
- 단점: 2가지 처리 경로 지원

카파kappa 아키텍처
- 일괄 처리와 스트리밍 처리 경로를 하나로 결합

### 데이터 재계산 historical data replay
![image](https://github.com/user-attachments/assets/c2cf4e88-4670-4b68-9271-e9687b4caf0c)

1. 원시 데이터 저장소에서 데이터 검색: 일괄 처리 프로세스
2. 추출된 데이터는 전용 집계 서비스로 전송: 실시간 처리 과정과의 간섭 방지
3. 집계 결과 두 번째 메시지 큐로 전송 및 DB 반영

### 시간

- 이벤트 시각: 클릭 발생 시각
- 처리 시각: 이벤트를 처리한 시스템 시각
![image](https://github.com/user-attachments/assets/0f7757fb-71fa-479c-bb8d-23c77f7ff07d)


데이터 정확도를 위해 이벤트 시각을 사용 
늦게 도착한 이벤트 처리 → 워터마크 기술 사용
![image](https://github.com/user-attachments/assets/ec2ee409-429c-40bf-8ea2-264e0e589c87)



텀블링 윈도우에 워터마크를 붙여 살짝 늦게 처리된 이벤트들도 같은 윈도우에서 집계될 수 있게 한다. (윈도1에서 2, 윈도3에서 5)

데이터 정확도는 높아지지만, 대기 시간이 늘어나 지연 시간이 늘어난다.

### 집계 윈도

- 텀블링 윈도 tumbling (= 고정 윈도 fixed)
    - 시간을 같은 크기의 겹치치 않는 구간으로 분할
- 호핑 윈도 hopping
- 슬라이딩 윈도 sliding
    - 데이터 스트림을 미끄러져 나가면서 같은 시간 구간 내 이벤트 집계 - 겹칠 수 있다.
- 세션 윈도 session

## 전달 보장

- 이벤트 중복 처리를 어떻게 피할 수 있는가?
- 모든 이벤트의 처리를 어떻게 보장할 수 있는가?

at-most once, at-least once, exactly once

데이터의 정확성이 중요하므로 exactly once방식 권장

### 데이터 중복 제거

- 클라이언트 측: 같은 이벤트를 여러 번 보내는 경우
    - 악의적 중복 이벤트 처리는 ad fraud / risk control 컴포넌트 활용
- 서버 장애: 집계 도중 장애로 업스트림 서비스가 응답을 받지 못해 재 전송

집계 서비스 노드 장애로 발생한 중복 데이터 처리 방법 → 오프셋을 외부에 저장 

이 때 다운스트림에서 집계 결과를 수신한 후 저장해야 데이터 손실을 막을 수 있다.

정확히 한 번 처리하기 위해서는 분산 트랜잭션을 활용할 수 있다.

## 시스템 규모 확장

매년 30%씩 성장

### 메시지 큐의 규모 확장

[4장 분산 메시지 큐](https://www.notion.so/4-7e4fb5ee20c448658722f7189159b404?pvs=21) 참고

- 소비자 확장시 재조정 작업이 오래 걸리므로 사용량이 많지 않을 때 진행

### 브로커 broker

- 해시 키: 같은 ad_id를 같은 파티션에 저장하기 위해 ad_id를 해시키로 사용
- 파티션의 수: 충분한 파티션 수를 확보해 같은 ad_id가 같은 파티션에 기록되도록 한다
- 토픽의 물리적 샤딩
    - 지역, 사업 유형에 따라 여러 토피을 둘 수 있다.
    - 장점: 처리 대역폭 증가, 그룹 재조정 시간 단축
    - 단점: 복잡성, 유지 관리 비용 증가

### 집계 서비스의 규모 확장

노드 추가/삭제로 수평적 조정 가능

![IMG_6F833924D7A5-1.jpeg](https://prod-files-secure.s3.us-west-2.amazonaws.com/ac122fbb-6382-4320-bf37-68e21a66352e/832c61ae-5ec4-490c-8bd6-f0b2854040de/IMG_6F833924D7A5-1.jpeg)

집계 서비스의 처리 대역폭을 높이려면?

#1 ad_id마다 별도 처리 스레드 두기

#2 집계 서비스 노드를 아파치 하둡 YARN과 같은 자원 공급자에 배포 → 다중 프로세싱 활용

### 데이터 베이스의 규모 확장

카산드라의 경우 안정 해시와 유사한 방식으로 수평적 규모확장을 지원한다

### 핫스팟 문제

![IMG_75162323C20C-1.jpeg](https://prod-files-secure.s3.us-west-2.amazonaws.com/ac122fbb-6382-4320-bf37-68e21a66352e/5910ff5f-dc9e-42df-b82d-04abd0cfa7bf/IMG_75162323C20C-1.jpeg)

1. 집계 서비스 노드에 처리 가능한 이벤트 수(100개) 초과하는 이벤트 도착 → 자원 관리자에 추가 자원 신청
2. 자원 관리자는 추가 자원 할당 (2개)
3. 원래 집계 서비스 노드를 처리 가능한 이벤트 수 만큼 분리 → 3개로
4. 집계가 끝나 축약된 결과를 원래 노드에 기록

### 결함 내성 fault tolerance

: 집계 노드 장애로 인한 결과 손실 처리

업스트림 오프셋 같은 ‘시스템 상태’를 스냅숏으로 저장하고, 마지막으로 저장된 상태부터 복구

## 데이터 모니터링 및 정확성

RTB 및 청구서 발행 목적으로 사용 가능

### 지속적 모니터링

- 지연 시간 latency: 시스템의 중요 부분마다 timestamp 추적이 가능하도록
- 메시지 큐 크기: 집계 서비스 노드 추가 여부 판단, 카프카킄 레코드 처리 지연 지표 추적
- 집계 노드의 시스템 자원: CPU, 디스크, JVM 등

### 조정 reconciliation

다양한 데이터를 비교해 무결성 보증

특정 시각에 일괄 처리해 만든 데이터와 실시간 집계 결과 비교 → 늦게 도착하는 이벤트로 인해 정확히 일치 하지 않을 수 있음 고려

## 대안적 설계안

- 광고 클릭 데이터를 하이브Hive에 저장 후 빠른 질의는 ES로 처리
- 집계는 클릭하우스ClickHouse나 드루이드Druid 같은 OLAP DB로 처리

# 4단계: 마무리

- 데이터 모델 및 API 설계
- 맵리듀스 데이터 처리 패러다임을 통해 광고 클릭 이벤트 집계
- 메시지 큐, 집계 서비스, 데이터베이스 규모 확장 방안
- 핫스팟 문제 해결 방안
- 시스템 지속적 모니터링
- 데이터 조정을 통한 정확성 보증 방안
- 결함 내성
