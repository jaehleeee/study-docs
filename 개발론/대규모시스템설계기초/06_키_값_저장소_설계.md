 # 6장 키-값 저장소 설계
 * 키-값 저장소는 키-값 데이터베이스라고 볼리는 비 관계형 데이터베이스이다.
 * 키-값 쌍에서 키는 유일해야 하며, 해당 키에 매달린 값은 키를 통해서만 접근할 수 있다.
 * 키는 일반 텍스트일 수도 있고, 해시 값일수도 있다.
 * 성능상의 이유로 키는 짧을수록 좋다.


## 문제 이해 및 설계 범위 확정
 * 완벽한 설계란 없다.
 * 읽기, 쓰기 그리고 메모리 사용량 사이에 어떤 균형을 찾고, 데이터의 일관성과 가용성 사이에서 타협적 결정을 내린 설계를 만들면 쓸만한 답안이다.

### 조건
<img src="https://github.com/jaehleeee/study-docs/assets/48814463/444ac64f-4332-4b85-824b-0f55ae96c707" width="500"/>


## 단일 서버 키-값 저장소
 * 가장 직관적인 방법은 키-값 쌍 전부를 메모리에 해시 테이블로 저장하는 것
 * 빠르지만 모든 데이터를 메모리 안에 두는 것은 불가능
 * 개선책
   * 데이터 압축
   * 자주 쓰이는 데이터만 메모리에 두고 나머지는 디스크 저장
 * 이렇게 개선해도 언젠가 한 대 서버로 부족한 때가 곧 찾아온다.

## 분산 키-값 저장소
 * 분산 키-값 저장소는 분산 해시 테이블 이라고도 불린다.
 * 분산 시스템을 설계할 때는 CAP 정리를 이해하고 있어야 한다.

### CAP 정리
 * CAP 정리는 데이터 일관성(Consistency), 가용성(Availability), 파티션 감내(Partition tolerange) 라는 3가지 요구사항을 동시에 만족하는 분산 시스템을 설계하는 것은 불가능하다는 정리다.
 * 데이터 일관성(Consistency) : 클라이언트는 어떤 노드에 접속해도 같은 데이터를 봐야한다.
 * 가용성(Availability) : 클라이언트는 일부 노드에 장애가 발생해도 항상 응답을 받을 수 있어야 한다.
 * 파티션 감내(Partition tolerange) : 파티션은 두 노드 사이에 통신 장애가 발생했음을 의미. 네트워크에 파티션이 생기더라도 시스템은 계속 동작해야함을 의미.
 * 이 3가지 사항 중 2가지를 충족하려면 반드시 1가지는 희생되어야 한다.
 * 실세계에서 통상 네트워크 장애는 피할 수 없으므로, 일관성과 가용성 둘 중 하나를 포기할 수 밖에 없다.

### 실 예시
<img src="https://github.com/jaehleeee/study-docs/assets/48814463/e8d3e8a7-38f3-4918-a317-2c6e2004741e" width="500"/>

#### 데이터 일관성을 선택한다면?
 * 데이터 불일치를 중단시키기 위해 n1, n2 쓰기 중단
 * 특히 은행권 시스템에서 데이터 일관성을 선택한다.
 * 상황이 해결될때까지 오류를 반환해야 한다.

#### 가용성을 선택한다면?
 * 설사 낡은 데이터를 반환하더라도 계속 읽기 연산 허용
 * n1, n2는 쓰기 허용
 * 파티션 문제가 해결되면 새 데이터를 n3에 전송

## 시스템 컴포넌트 살펴보기
### 데이터 파티션 
 * 데이터를 파티션 단위로 나눌 때 해야할 2가지 질문
   * 데이터를 고르게 분산할 수 있는가
   * 노드가 추가, 삭제될때 데이터 이동을 최소화할 수 있는가.
 * 5장의 안정 해시가 이 문제를 푸는데 적합한 기술이다.
   * 규모 확장 자동화 : 시스템 부하에 따라 서버가 자동으로 추가, 삭제되도록 할 수 있다.
   * 다양성 : 각 서버의 용량에 맞게 가상 노드 수를 조정할 수 있다.
  
### 데이터 다중화
 * 높은 가용성과 안정성을 확보하기 위해서는 데이터를 N개 서버에 비동기적으로 다중화할 필요가 있다.
 * 안정해시를 활용하여, 링을 순회하면서 만나는 첫 N개 서버에 데이터 사본을 보관하는 것이다.
 * 다만, 가상 노드 사용시 N개보다 실제로 더 복사되는 서버 수는 작을 수 있다. 이 문제를 피하려면 노드 선택시 같은 물리 서버를 중복 선택하지 않도록 해야 한다.
### 데이터 일관성
 * 여러 노드에 다중화된 데이터는 적절히 동기화 되어야 한다.
 * Quorum consensus 프로토콜을 사용하면 읽기/쓰기 연산 모두에 일관성을 보장할 수 있다.
 * 사본 수(N) / 쓰기 연산 정족 수(W) / 읽기 연산 정족 수(R)
   * 정족 수 란? 요청에 대해 연산 성공했다는 서버의 응답을 받는 수
   * W + R > N : 강한 일관성
   * W + R  < N : 강한 일관성 보장 X
#### 비 일관성 해소 기법 : 데이터 버져닝
 * 데이터 다중화하면 가용성은 높아지지만, 사본 간 일관성은 깨질 가능성이 높아진다.
 * 버저닝과 벡터 시계는 이 문제를 해소하기 위해 등장
 * 데이터 변경할때마다 해당 데이터의 새로운 버전을 만드는 것이다.
 * 따라서 각 버젼의 데이터는 변경 불가
 * 만약 2개의 서버에서 동시에 한 데이터에 대한 변경이 발생했다면?
   * 충돌을 발견하고 자동으로 해결해 낼 버저닝 시스템 필요 (벡터 시계)
   * 충돌을 감지하면, 클라이언트가 충돌을 해소한 후 서버에 기록해야 한다.
 * 단점은 버저닝(+벡터시계) 개수가 빠르게 증가할 수 있다. 일정 임계치 설정 후 이 수치 이상 길어지면 오래된 버져닝(+벡터시계)은 삭제해야한다.

### 장애 처리
#### 장애 감지
 * 한대 서버가 `지금 서버 A가 죽었습니다.` 라고 해서 바로 서버 A를 장애처리하지 않는다. 2대 이상 서버가 똑같이 보고해야 장애로 간주한다.
 * 모든 노드 사이에 멀티캐스팅 채널 구축하는 것이 서버 장애 감지에 가장 쉽지만 비효율적이다.
 * `가십 프로토콜` 같은 분산형 장애 감지 솔루션을 채택하는 편이 더 효율적
   * 각 노드는 멤버립 목록을 유지하는데, 여기엔 각 멤버아이디와 하트비트 counter 쌍을 기록한다.
   * 각 노드는 주기적으로 하트비트를 증가시킨다.
   * 각 노드는 무작위로 선정된 노드들에게 자기 하트비트 카운터 목록을 보낸다.
   * 하트비트 목록을 받으면 최산값으로 갱신한다.
   * 어떤 멤버의 하트비트 카운터 값이 갱신되지 않으면 그 서버를 장애 상태로 간주한다.
     
#### 일시적 장애 처리
 * 강한 일관성 이라면, 읽기와 쓰기 연산을 금지해야 한다.
 * 네트워크나 서버 문제로 장애 상태인 서버로 가는 요청은 다른 서버가 잠시 맡아 처리한다.
   * 복구까지 쌓인 데이터 변경사항은 복구 후 일괄 반영한다.
   * 이를 위해 그 사이 쌓인 데이터에는 hint를 남겨둔다.
#### 영구적 장애 처리
 * 반-엔트로피 프로토콜을 구현하여 사본들을 동기화 한다.
   * 이 과정은 사본들을 비교하여 최신 버전으로 갱신하는 과정을 포함한다.
   * 사본 간의 일관성이 망가진 상태를 탐지하고 전송 데이터의 양을 줄이기 위해서는 머클 트리를 사용할 것이다.
   * 머클트리는 해시 트리라고도 불린다. 각 노드에 그 자식 노드들에 보관된 값의 해시 또는 자식 노드들의 레이블로부터 계산된 해시 값을 레이블로 붙여두는 트리.
   * 루트 노드의 해시 값이 일치하면 두 서버는 같은 서버를 갖는다.

### 데이터 센터 장애처리
 * 데이터를 여러 데이터 센터에 다중화하는 것이 중요하다.



## 시스템 아키텍처 다이어그램
 * 완전히 분산된 설계에서 모든 노드는 아래 기능들을 모두 제공해야 한다.
   * 클라이언트 API
   * 장애 감지
   * 데이터 충돌 해소
   * 장애 복구 매커니즘
   * 다중화
   * 저장소 엔진

### 쓰기 경로
 * 쓰기 요청이 커밋 로그 파일(디스크) 기록
 * 데이터를 메모리 캐시에 기록
 * 메모리 캐시가 가득차거나 임게치 도달하면 데이터를 디스크에 SSTable에기록

### 읽기 경로
 * 메모리 캐시 먼저 조회
 * 없다면 디스크로 조회해야 하는데, 데이터가 있는지 없는지 먼저 확실히 알기 위해 블룸 필터 사용.
 * 블룸 필터를 통해 어떤 SSTable에 키가 보관되어 있는지 알아낸다.
 * SSTable에서 데이터 가져온다.
 * 클라이언트에게 반환한다.




