# 9장 웹 크롤러 설계
### 웹 크롤러란?
 * 로봇 또는 스파이더라고도 불림
 * 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주된 목적
 * 몇개 웹 페이지에서 시작하여 그 링크를 따라 나가면서 개로운 콘텐츠를 수집

<img src="https://github.com/jaehleeee/study-docs/assets/48814463/10276888-7473-4d73-9a9c-40968bc88246" width="500"/>

### 활용 사례
 * 검색 엔진 인덱싱
 * 웹 아카이빙
 * 웹 마이닝
 * 웹 모니터링

## 1단계. 문제 이해 및 설계 범위 확정
### 웹 크롤러의 기본 알고리즘
 * URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 모든 웹 페이지를 다운로드
 * 다운로드 받은 웹 페이지에서 URL 추출
 * 추출된 URL들을 다운로드 할 목록으로 추가하고 1~3 반복

### 설계 범위
 * 목적 : 검색 엔진 인덱스 생성용
 * 매달 10억개 웹페이지 수집
 * 수집한 웹 페이지 5년  저장
 * 중복은 무시

### 개략적 규모 추정 : 10억개 웹이지 저장하라면?
 * 월 10억개면 -> 1초 400페이지 -> QPS 400
 * 1 페이지 크기 평균을 500k 로 가정
 * 10억 * 500k = 500TB/월
 * 5년 저장하려면 30PB 저장용량 필요

## 2단계. 개략적 설계안 제시 및 동의 구하기

<img src="https://github.com/jaehleeee/study-docs/assets/48814463/193cc752-a440-45ff-a11f-11fe4e4d533c" width="500"/>

### 시작 URL 집합
 * 크롤러 시작 출발점
 * 일반적으로 작은 부분 집합으로 나누는 전략을 쓴다. 국가나 지역별로 인기 있는 웹사이트가 다르므로.
 * 정답이 없으니, 정한 것의 의도가 무엇인지만 정확히 전달하자.
### 미수집 URL 저장소
 * FIFO 큐
#### 수집 한 주소 중 같은 페이지를 가리키는 주소가 몇 개 있다. 이 중 메인 주소를 선택하는 방법 : canonical tag
`<link rel="canonical" href='http://www.example.com/index.html'>`
 * 검색 엔진이 자체적으로 캐노니컬 태그를 붙이거나
 * 메인 검색 엔진 (야후, 구글 등) 해당 페이지의 작성자가 붙이도록 HTML 문서 HEAD 에 추가하는 방식을 제공
### HTML 다운로더
### 도메인 이름 변환기
 * URL을 IP로 변환
### 콘텐츠 파서
 * 파싱과 검증 절차
### 중복 컨텐츠 체크
 * 웹 공개 연구에 따르면 29% 웹 페이지 컨텐츠는 중복이다.
 * 웹페이지 해시값 비교를 활용하여 중복을 해결하자.
### URL 추출기
 * HTML을 파싱하여 링크들을 골라내는 역할
### URL 필터
 * 접속시 오류가 발생하는 URL, 접근 제외 목록 포함 URL 등을 배제하는 역할
### URL 이미 방문한 URL 체크
 * 이미 방문한 URL 이나 미수집 URL 저장소에 보관된 URL을 추적하는 자료구조 사용
 * 불룸피렅나 해시테이블을 널리 사용.
### URL 저장소
 * 이미 방문한 URL 을 보관

## 3단계 상세 설계

### DFS vs BFS
* 웹크롤러에서는 보통 BFS를 사용한다. (DFS를 쓰게 될때 얼마나 깊은 탐색까지 이어질지 모르므로
* BFS 사용시 문제점
  * 한 페이지에서 나오는 링크는 상당 수 같은 서버로 되돌아가게 되면서 한 서버에 요청이 과부하걸리게 되어 `예의없는 크롤러`가 된다.
  * 페이지간 우선순위 없이 조회하게 된다. (실제로는 사용자 트래픽, 업데이트 빈도 등 페이지간 우선순위가 있다.)

### 미수집 URL 저장소
 * 이 저장소를 잘 구현하면 예의를 갖춘 크롤러가될 수 있다.
 * 즉, URL 사이에 우선순위와 신선도를 구별하는 크롤러를 구현할 수 있다.
#### 예의
 * 돌일 웹 사이트에 대해 한번에 한 페이지만 요청하도록 한다.
 * 같은 웹사이트에 대한 다운로드 태스크를 시간차를 두고 실행한다.
 * 설계
   * 큐라우터를 통해 같은 호스트는 같은 큐로 가도록 라우팅 (호스트-큐 매핑 테이블 활용)
   * 같은 호스트에 속한 url은 언제나 같은 큐에 보관
   * 큐 선택기 : 순회하면서 큐에서 URL을 거내 url을 작업 스레드로 전달.

<img src="https://github.com/jaehleeee/study-docs/assets/48814463/7c1c73ff-8325-4c7c-8c8f-06e558462e9a" width="500"/>

#### 우선순위
 * 페이지랭크, 트래픽양, 갱신 빈도 등을 사용가능하다. 이를 순위결정장치에로 판단한다.
 * 우선순위별로 큐가 하나씩 할당된다.
 * 큐 선택기 : 순위가 높은 큐에서 좀 더 자주 꺼낸다.

<img src="https://github.com/jaehleeee/study-docs/assets/48814463/3d2a5587-a4f5-4d45-b724-3b10f08b24c6" width="500"/>

#### 신선도
 * 이미 다운로드한 페이지라도 주기적으로 재수집할 필요가 있다.
 * 신선도 판단을 위한 전략
   * 페이지 변경 이력 활용
   * 우선순위를 활용하여, 중요 페이지는 좀 더 자주 재수집 

### HTML 다운로더
 * 다운로더에 중요한 요소 : Robots.txt와 성능 최적화, 안정성
#### Robots.txt
 * 로봇 제외 프로토콜이라고 불림.
 * 웹사이트가 크롤러와 소통하는 방법
 * 네이버 robots.txt 설정 가이드 (https://searchadvisor.naver.com/guide/seo-basic-robots)
 * 예시
   * `www.naver.com/robots.txt` : 사이트의 루트 페이지만 수집 허용
   * `https://www.google.com/robots.txt` : 구글은 매우 디테일함.
#### 성능 최적화 기법들
 * 분산 크롤링 : 크롤링 작업을 여러 서버에 분산
 * 도메인 이름 변환 결과를 캐싱해둔다.
 * 지역성 : 크롤링 서버가 다운로드할 페이지와 가까울수록 빠르다.
 * 짧은 타임아웃
#### 안정성
 * 안정해시 - 다운로더 서버 부하 분산 및 쉽게 추가하고 삭제
 * 크롤링 상태 및 수집 데이터 저장 - 장애 발생해도 쉽게 복구할 수 있도록 크롤링 상태와 수집 데이터를 지속 저장
 * 예외 처리 - 에러 발생해도 전체 시스템 중단되지 않게
 * 데이터 검증 - ?


### 문제 있는 컨텐츠 감지 회피
 * 중복 컨텐츠 - 해시나 체크섬 활용
 * 거미 덫 - 무한루프 빠뜨린다. 수작업으로 덫을 확인하여 크롤러 탐색 대상에서 제외해야 한다.
 * 데이터 노이즈 - 가치가 없는 콘텐츠는 가능한 제외한다.

## 4단계 마무리 : 더 얘기해볼 주제
 * 서버 측 렌더링
 * 원치 않는 페이지 필터링
 * DB 다중화 및 샤딩
 * 수평적 규모 확장
 * 가용성, 일관성, 안정성
 * 데이터 분석 솔루션


## 참고
 * 네이버 https://searchadvisor.naver.com/guide
 * 구글 https://developers.google.com/search/docs/fundamentals/seo-starter-guide?hl=ko

